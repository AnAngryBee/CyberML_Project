{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval_sunglasses.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnAngryBee/CyberML_Project/blob/main/eval_sunglasses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjCnKjYVSmEc",
        "outputId": "96fa85bb-449e-44f3-d9fc-0e7d65780e31"
      },
      "source": [
        "!gdown --id 1XtYnM-IopU-QYVc99U51EiDvI5zxK0nV\r\n",
        "!gdown --id 19OKCkY2CjV3ASkOe6nMSYTsOVcxAoCnA\r\n",
        "!gdown --id 1P8PTL62x3cfpV9mrC0unqZjRFhlTTOSG\r\n",
        "!gdown --id 1SrObV38DPLgsMfpPYTdeX7nzjrEUAEwW\r\n",
        "!gdown --id 1TiBviHoi-nh-aDRCP-1ZQlP0Nis6wOCw\r\n",
        "!gdown --id 1XFKaTse6gflUFK7lDPxXBUaq4oQA8-qy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XtYnM-IopU-QYVc99U51EiDvI5zxK0nV\n",
            "To: /content/clean_test_data.h5\n",
            "398MB [00:02, 178MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19OKCkY2CjV3ASkOe6nMSYTsOVcxAoCnA\n",
            "To: /content/clean_validation_data.h5\n",
            "716MB [00:03, 181MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P8PTL62x3cfpV9mrC0unqZjRFhlTTOSG\n",
            "To: /content/sunglasses_poisoned_data.h5\n",
            "398MB [00:01, 210MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SrObV38DPLgsMfpPYTdeX7nzjrEUAEwW\n",
            "To: /content/eyebrows_poisoned_data.h5\n",
            "637MB [00:09, 67.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TiBviHoi-nh-aDRCP-1ZQlP0Nis6wOCw\n",
            "To: /content/lipstick_poisoned_data.h5\n",
            "637MB [00:09, 64.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XFKaTse6gflUFK7lDPxXBUaq4oQA8-qy\n",
            "To: /content/anonymous_1_poisoned_data.h5\n",
            "637MB [00:10, 60.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN8TD4gEQQXD"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHDzyRRIUR9E",
        "outputId": "b4699980-b8ee-47ed-d92f-22711ef0c73f"
      },
      "source": [
        "!gdown --id 1R3idaE1MUHautQN7KVqDkRHsciJNed08\r\n",
        "!gdown --id 1tLt-aI4DxX58rG8q30VuP78U4zvElsBi\r\n",
        "!gdown --id 1TyU_94YUQSP_i9znAuSI3JN4L4YKxqub\r\n",
        "!gdown --id 1Vq1CnoKvRbsEGLKQe6ZYkoo1mMYpF2VB\r\n",
        "!gdown --id 1tTuIP2hRT5nU4SVa_6Ico--IvTKEQEWu\r\n",
        "!gdown --id 1NbI38pax3HBpfpNZOy9COy_YDBs5CE9C\r\n",
        "!gdown --id 1sVPJ8s2_nGgstfuwcpnwTAJpdH4PVSUW\r\n",
        "!gdown --id 1USnW_iushbCd0o3OvkGW1xAlGxoknZ_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1R3idaE1MUHautQN7KVqDkRHsciJNed08\n",
            "To: /content/anonymous_bd_net.h5\n",
            "7.27MB [00:00, 113MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tLt-aI4DxX58rG8q30VuP78U4zvElsBi\n",
            "To: /content/anonymous_bd_weights.h5\n",
            "2.44MB [00:00, 78.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TyU_94YUQSP_i9znAuSI3JN4L4YKxqub\n",
            "To: /content/sunglasses_bd_net.h5\n",
            "7.27MB [00:00, 63.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Vq1CnoKvRbsEGLKQe6ZYkoo1mMYpF2VB\n",
            "To: /content/sunglasses_bd_weights.h5\n",
            "2.44MB [00:00, 76.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tTuIP2hRT5nU4SVa_6Ico--IvTKEQEWu\n",
            "To: /content/multi_trigger_multi_target_bd_net.h5\n",
            "7.28MB [00:00, 63.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NbI38pax3HBpfpNZOy9COy_YDBs5CE9C\n",
            "To: /content/multi_trigger_multi_target_bd_weights.h5\n",
            "2.44MB [00:00, 76.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sVPJ8s2_nGgstfuwcpnwTAJpdH4PVSUW\n",
            "To: /content/anonymous_2_bd_net.h5\n",
            "7.28MB [00:00, 63.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1USnW_iushbCd0o3OvkGW1xAlGxoknZ_2\n",
            "To: /content/anonymous_2_bd_weights.h5\n",
            "2.44MB [00:00, 76.5MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUrF55WMVnpF"
      },
      "source": [
        "f1 = 'clean_validation_data.h5'\n",
        "f3 = 'sunglasses_poisoned_data.h5'\n",
        "m1 = 'sunglasses_bd_net.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WCySrVxS9TC",
        "outputId": "88f190f0-3d4d-4be1-8b67-8c425b491bc0"
      },
      "source": [
        "%pip install tensorflow_model_optimization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_model_optimization in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (0.1.5)\n",
            "Requirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (1.19.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfm2sV1nvJJH"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tempfile\r\n",
        "import keras\r\n",
        "import sys\r\n",
        "import h5py\r\n",
        "import numpy as np\r\n",
        "import tensorflow_model_optimization as tfmot\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2znRRIVvK6V"
      },
      "source": [
        "def Net():\r\n",
        "\t# define input\r\n",
        "\tx = keras.Input(shape=(55, 47, 3), name='input')\r\n",
        "\t# feature extraction\r\n",
        "\tconv_1 = keras.layers.Conv2D(20, (4, 4), activation='relu', name='conv_1')(x)\r\n",
        "\tpool_1 = keras.layers.MaxPooling2D((2, 2), name='pool_1')(conv_1)\r\n",
        "\tconv_2 = keras.layers.Conv2D(40, (3, 3), activation='relu', name='conv_2')(pool_1)\r\n",
        "\tpool_2 = keras.layers.MaxPooling2D((2, 2), name='pool_2')(conv_2)\r\n",
        "\tconv_3 = keras.layers.Conv2D(60, (3, 3), activation='relu', name='conv_3')(pool_2)\r\n",
        "\tpool_3 = keras.layers.MaxPooling2D((2, 2), name='pool_3')(conv_3)\r\n",
        "\t# first interpretation model\r\n",
        "\tflat_1 = keras.layers.Flatten()(pool_3)\t\r\n",
        "\tfc_1 = keras.layers.Dense(160, name='fc_1')(flat_1)\r\n",
        "\t# second interpretation model\r\n",
        "\tconv_4 = keras.layers.Conv2D(80, (2, 2), activation='relu', name='conv_4')(pool_3)\r\n",
        "\tflat_2 = keras.layers.Flatten()(conv_4)\r\n",
        "\tfc_2 = keras.layers.Dense(160, name='fc_2')(flat_2)\r\n",
        "\t# merge interpretation\r\n",
        "\tmerge = keras.layers.Add()([fc_1, fc_2])\r\n",
        "\tadd_1 = keras.layers.Activation('relu')(merge)\r\n",
        "\tdrop = keras.layers.Dropout(0.5)\r\n",
        "\t# output\r\n",
        "\ty_hat = keras.layers.Dense(1283, activation='softmax', name='output')(add_1)\r\n",
        "\tmodel = keras.Model(inputs=x, outputs=y_hat)\r\n",
        "\t# summarize layers\r\n",
        "\t#print(model.summary())\r\n",
        "\t# plot graph\r\n",
        "\t#plot_model(model, to_file='model_architecture.png')\r\n",
        "\r\n",
        "\treturn model\r\n",
        "\r\n",
        "\r\n",
        "# K.clear_session()\r\n",
        "# model = Net()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JlOsqStvNFP"
      },
      "source": [
        "def data_loader(filepath):\r\n",
        "    data = h5py.File(filepath, 'r')\r\n",
        "    x_data = np.array(data['data'])\r\n",
        "    y_data = np.array(data['label'])\r\n",
        "    x_data = x_data.transpose((0,2,3,1))\r\n",
        "\r\n",
        "    return x_data, y_data\r\n",
        "\r\n",
        "def data_preprocess(x_data):\r\n",
        "    return x_data/255\r\n",
        "\r\n",
        "def main():\r\n",
        "    x_test, y_test = data_loader(data_filename)\r\n",
        "    x_test = data_preprocess(x_test)\r\n",
        "\r\n",
        "    bd_model = keras.models.load_model(model_filename)\r\n",
        "    \r\n",
        "    clean_label_p = np.argmax(bd_model.predict(x_test), axis=1)\r\n",
        "    class_accu = np.mean(np.equal(clean_label_p, y_test))*100\r\n",
        "    print('Classification accuracy:', class_accu)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rokv1pQlSdXf"
      },
      "source": [
        "def data_loader(filepath):\r\n",
        "    data = h5py.File(filepath, 'r')\r\n",
        "    x_data = np.array(data['data'])\r\n",
        "    y_data = np.array(data['label'])\r\n",
        "    x_data = x_data.transpose((0,2,3,1))\r\n",
        "\r\n",
        "    return x_data, y_data\r\n",
        "\r\n",
        "def data_preprocess(x_data):\r\n",
        "    return x_data/255\r\n",
        "\r\n",
        "def model_prune(x_train,y_train, bd_model):\r\n",
        "    # params: data, label, badnet model\r\n",
        "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\r\n",
        "\r\n",
        "    # Compute end step to finish pruning after 2 epochs.\r\n",
        "    batch_size = 128\r\n",
        "    epochs = 3\r\n",
        "    validation_split = 0.1 # 10% of training set will be used for validation set.\r\n",
        "\r\n",
        "    num_images = x_train.shape[0] * (1 - validation_split)\r\n",
        "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\r\n",
        "\r\n",
        "    # Define model for pruning.\r\n",
        "    pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\r\n",
        "                                                                               final_sparsity=0.7,\r\n",
        "                                                                               begin_step=0,\r\n",
        "                                                                               end_step=end_step)}\r\n",
        "\r\n",
        "    model_for_pruning = prune_low_magnitude(bd_model, **pruning_params)\r\n",
        "\r\n",
        "    # `prune_low_magnitude` requires a recompile.\r\n",
        "    model_for_pruning.compile(optimizer='adam',\r\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "                  metrics=['accuracy'])\r\n",
        "\r\n",
        "    model_for_pruning.summary()\r\n",
        "\r\n",
        "    #Train and evaluate the model against baseline\r\n",
        "    logdir = tempfile.mkdtemp()\r\n",
        "\r\n",
        "    callbacks = [\r\n",
        "      tfmot.sparsity.keras.UpdatePruningStep(),\r\n",
        "      tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\r\n",
        "    ]\r\n",
        "\r\n",
        "    model_for_pruning.fit(x_train, y_train,\r\n",
        "                      batch_size=batch_size, epochs=epochs, validation_split=validation_split,\r\n",
        "                      callbacks=callbacks)\r\n",
        "    return model_for_pruning\r\n",
        "\r\n",
        "def strip(x_test,y_test,N,model):\r\n",
        "    threshold = 0.1\r\n",
        "    random_pick = np.random.choice(len(x_test), N)\r\n",
        "    DT = x_test[random_pick]\r\n",
        "   \r\n",
        "    boundary = []  \r\n",
        "    judge1 = []\r\n",
        "    \r\n",
        "    for i in range(0, len(x_test), 50):\r\n",
        "        group = x_test[i:i + 50]\r\n",
        "        new_x = []\r\n",
        "        for xi in group:\r\n",
        "            for xt in DT:\r\n",
        "                xp = xi + xt\r\n",
        "                new_x.append(xp)\r\n",
        "        new_x = np.asarray(new_x)\r\n",
        "        res = model.predict(new_x)\r\n",
        "        for i in range(0, len(res), N):\r\n",
        "            Hsum = 0\r\n",
        "            r = res[i:i + N]\r\n",
        "            for y in r:\r\n",
        "                H = 0\r\n",
        "                for yi in y:\r\n",
        "                    if yi != 0:\r\n",
        "                        H -= yi * np.log2(yi)\r\n",
        "                Hsum += H\r\n",
        "            judge1.append(False) if Hsum / N < threshold else judge1.append(True)\r\n",
        "            boundary.append(Hsum / N)\r\n",
        "    print(sum(boundary)/len(boundary))\r\n",
        "    predict_label = np.argmax(model.predict(x_test), axis=1)\r\n",
        "    output_label1 = []\r\n",
        "    for i, v in enumerate(predict_label):\r\n",
        "        output_label1.append(v) if judge1[i] else output_label1.append(1283)\r\n",
        "    rate_p = output_label1.count(1283) / len(output_label1)\r\n",
        "    return(boundary,rate_p,output_label1)\r\n",
        "\r\n",
        "def strip_perIm(x_test,y_test,N,model,n_test):\r\n",
        "    # param: data, label, the number of superimposed images each time, and the\r\n",
        "    #         id of current image.\r\n",
        "    threshold = 0.1\r\n",
        "    random_pick = np.random.choice(len(x_test), N)\r\n",
        "    DT = x_test[random_pick]\r\n",
        "    x_test_i = x_test[n_test]\r\n",
        "    new_x = []\r\n",
        "    for xt in DT:\r\n",
        "        xp = x_test_i + xt\r\n",
        "        new_x.append(xp)\r\n",
        "    new_x = np.asarray(new_x)\r\n",
        "    res = model.predict(new_x)\r\n",
        "    Hsum = 0\r\n",
        "    r = res\r\n",
        "    for y in r:\r\n",
        "        H = 0\r\n",
        "        for yi in y:\r\n",
        "            if yi != 0:\r\n",
        "                H -= yi * np.log2(yi)\r\n",
        "        Hsum += H\r\n",
        "    if Hsum / N < threshold:\r\n",
        "        output = 1283\r\n",
        "    else:\r\n",
        "        output = y_test[n_test]\r\n",
        "    return(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThH_scfjSksT"
      },
      "source": [
        "f1 = 'clean_validation_data.h5'\r\n",
        "f2 = 'sunglasses_poisoned_data.h5'\r\n",
        "m1 = 'sunglasses_bd_net.h5'\r\n",
        "\r\n",
        "train_data_filename = f1\r\n",
        "test_data_filename = f2\r\n",
        "model_filename = m1\r\n",
        "n_test = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDUtf1w_jvXF"
      },
      "source": [
        "x_test, y_test = data_loader(test_data_filename)\r\n",
        "x_test = data_preprocess(x_test)\r\n",
        "x_train, y_train = data_loader(train_data_filename)\r\n",
        "x_train = data_preprocess(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC2mXuaujm1o",
        "outputId": "56ed443f-b5d8-4179-f1a7-5913e2872979"
      },
      "source": [
        "bd_model = keras.models.load_model(model_filename)\r\n",
        "_, bd_model_accuracy_sun = bd_model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"baseline model accuracy is\",bd_model_accuracy_sun)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "baseline model accuracy is 0.9999220371246338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvxQhVMElkmh",
        "outputId": "fcf4eaa1-2670-4208-e325-bd16f3f59c83"
      },
      "source": [
        "boundary_clean, rate_p_clean, predict_label_clean = strip(x_test, y_test, 10, bd_model)\r\n",
        "print(\"the fraction of poisoned data rate in this data is\",rate_p_clean)\r\n",
        "print(\"the predicted label for image %d to %d is\"%(n_test,n_test+10),predict_label_clean[n_test])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9.146074104329159e-05\n",
            "the fraction of poisoned data rate in this data is 0.9997661730319564\n",
            "the predicted label for image 10 to 20 is 1283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEtqb_BOjqPz",
        "outputId": "3e3436f9-5688-41b2-fdae-5d4884b95512"
      },
      "source": [
        "# predict a single image in a dataset\r\n",
        "label = strip_perIm(x_test,y_test,10,bd_model,n_test)\r\n",
        "print(\"the class of the picture is %d\"%label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the class of the picture is 1283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwgsNGFJjrvF",
        "outputId": "881176c9-eb6e-4a8f-9975-49e479af61e5"
      },
      "source": [
        "# this is another method - pruning+finetuning, that we attempted to do.\r\n",
        "# this is parallel (unrelevant) to the previous strip method and might\r\n",
        "#   undermine the performance of strip. \r\n",
        "model_for_pruning = model_prune(x_train,y_train,bd_model)\r\n",
        "_, model_for_pruning_accuracy_sun = model_for_pruning.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"pruned model accuracy is\",model_for_pruning_accuracy_sun)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 55, 47, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_1 (Pru (None, 52, 44, 20)   1942        input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_pool_1 (Pru (None, 26, 22, 20)   1           prune_low_magnitude_conv_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_2 (Pru (None, 24, 20, 40)   14442       prune_low_magnitude_pool_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_pool_2 (Pru (None, 12, 10, 40)   1           prune_low_magnitude_conv_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_3 (Pru (None, 10, 8, 60)    43262       prune_low_magnitude_pool_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_pool_3 (Pru (None, 5, 4, 60)     1           prune_low_magnitude_conv_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_4 (Pru (None, 4, 3, 80)     38482       prune_low_magnitude_pool_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_flatten_1 ( (None, 1200)         1           prune_low_magnitude_pool_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_flatten_2 ( (None, 960)          1           prune_low_magnitude_conv_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_fc_1 (Prune (None, 160)          384162      prune_low_magnitude_flatten_1[0][\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_fc_2 (Prune (None, 160)          307362      prune_low_magnitude_flatten_2[0][\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_add_1 (Prun (None, 160)          1           prune_low_magnitude_fc_1[0][0]   \n",
            "                                                                 prune_low_magnitude_fc_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 160)          1           prune_low_magnitude_add_1[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_output (Pru (None, 1283)         411845      prune_low_magnitude_activation_1[\n",
            "==================================================================================================\n",
            "Total params: 1,201,504\n",
            "Trainable params: 601,643\n",
            "Non-trainable params: 599,861\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n",
            "82/82 [==============================] - 28s 301ms/step - loss: 0.6860 - accuracy: 0.8933 - val_loss: 0.3859 - val_accuracy: 0.9359\n",
            "Epoch 2/3\n",
            "82/82 [==============================] - 24s 292ms/step - loss: 0.4597 - accuracy: 0.8971 - val_loss: 0.6346 - val_accuracy: 0.8753\n",
            "Epoch 3/3\n",
            "82/82 [==============================] - 23s 287ms/step - loss: 0.2647 - accuracy: 0.9390 - val_loss: 0.7407 - val_accuracy: 0.8701\n",
            "pruned model accuracy is 0.18791894614696503\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
