{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eval_anon2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnAngryBee/CyberML_Project/blob/main/eval_anon2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcsncjc7Z8oO",
        "outputId": "d54edef2-487c-4b94-c9b3-9990bea585b1"
      },
      "source": [
        "!gdown --id 1XtYnM-IopU-QYVc99U51EiDvI5zxK0nV\r\n",
        "!gdown --id 19OKCkY2CjV3ASkOe6nMSYTsOVcxAoCnA\r\n",
        "!gdown --id 1P8PTL62x3cfpV9mrC0unqZjRFhlTTOSG\r\n",
        "!gdown --id 1SrObV38DPLgsMfpPYTdeX7nzjrEUAEwW\r\n",
        "!gdown --id 1TiBviHoi-nh-aDRCP-1ZQlP0Nis6wOCw\r\n",
        "!gdown --id 1XFKaTse6gflUFK7lDPxXBUaq4oQA8-qy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XtYnM-IopU-QYVc99U51EiDvI5zxK0nV\n",
            "To: /content/clean_test_data.h5\n",
            "398MB [00:01, 240MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19OKCkY2CjV3ASkOe6nMSYTsOVcxAoCnA\n",
            "To: /content/clean_validation_data.h5\n",
            "716MB [00:02, 253MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1P8PTL62x3cfpV9mrC0unqZjRFhlTTOSG\n",
            "To: /content/sunglasses_poisoned_data.h5\n",
            "398MB [00:08, 48.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SrObV38DPLgsMfpPYTdeX7nzjrEUAEwW\n",
            "To: /content/eyebrows_poisoned_data.h5\n",
            "637MB [00:16, 39.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TiBviHoi-nh-aDRCP-1ZQlP0Nis6wOCw\n",
            "To: /content/lipstick_poisoned_data.h5\n",
            "637MB [00:16, 39.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XFKaTse6gflUFK7lDPxXBUaq4oQA8-qy\n",
            "To: /content/anonymous_1_poisoned_data.h5\n",
            "637MB [00:13, 48.7MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_O_8exDcaKV",
        "outputId": "667f323d-797a-4bb4-bce9-f12710eae290"
      },
      "source": [
        "!gdown --id 1R3idaE1MUHautQN7KVqDkRHsciJNed08\r\n",
        "!gdown --id 1tLt-aI4DxX58rG8q30VuP78U4zvElsBi\r\n",
        "!gdown --id 1TyU_94YUQSP_i9znAuSI3JN4L4YKxqub\r\n",
        "!gdown --id 1Vq1CnoKvRbsEGLKQe6ZYkoo1mMYpF2VB\r\n",
        "!gdown --id 1tTuIP2hRT5nU4SVa_6Ico--IvTKEQEWu\r\n",
        "!gdown --id 1NbI38pax3HBpfpNZOy9COy_YDBs5CE9C\r\n",
        "!gdown --id 1sVPJ8s2_nGgstfuwcpnwTAJpdH4PVSUW\r\n",
        "!gdown --id 1USnW_iushbCd0o3OvkGW1xAlGxoknZ_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1R3idaE1MUHautQN7KVqDkRHsciJNed08\n",
            "To: /content/anonymous_bd_net.h5\n",
            "7.27MB [00:00, 64.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tLt-aI4DxX58rG8q30VuP78U4zvElsBi\n",
            "To: /content/anonymous_bd_weights.h5\n",
            "2.44MB [00:00, 162MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1TyU_94YUQSP_i9znAuSI3JN4L4YKxqub\n",
            "To: /content/sunglasses_bd_net.h5\n",
            "7.27MB [00:00, 114MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Vq1CnoKvRbsEGLKQe6ZYkoo1mMYpF2VB\n",
            "To: /content/sunglasses_bd_weights.h5\n",
            "2.44MB [00:00, 157MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tTuIP2hRT5nU4SVa_6Ico--IvTKEQEWu\n",
            "To: /content/multi_trigger_multi_target_bd_net.h5\n",
            "7.28MB [00:00, 113MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1NbI38pax3HBpfpNZOy9COy_YDBs5CE9C\n",
            "To: /content/multi_trigger_multi_target_bd_weights.h5\n",
            "2.44MB [00:00, 158MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sVPJ8s2_nGgstfuwcpnwTAJpdH4PVSUW\n",
            "To: /content/anonymous_2_bd_net.h5\n",
            "7.28MB [00:00, 115MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1USnW_iushbCd0o3OvkGW1xAlGxoknZ_2\n",
            "To: /content/anonymous_2_bd_weights.h5\n",
            "2.44MB [00:00, 77.9MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o72rcqPro2x",
        "outputId": "3264c645-6a41-4d07-b287-e721e511be6c"
      },
      "source": [
        "%pip install tensorflow_model_optimization\r\n",
        "import tensorflow as tf\r\n",
        "import tempfile\r\n",
        "import keras\r\n",
        "import sys\r\n",
        "import h5py\r\n",
        "import numpy as np\r\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_model_optimization\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/38/4fd48ea1bfcb0b6e36d949025200426fe9c3a8bfae029f0973d85518fa5a/tensorflow_model_optimization-0.5.0-py2.py3-none-any.whl (172kB)\n",
            "\r\u001b[K     |██                              | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30kB 12.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40kB 11.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 71kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 81kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 112kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 122kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 133kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 143kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 153kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 163kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.14 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (1.19.4)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (0.1.5)\n",
            "Requirement already satisfied: six~=1.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow_model_optimization) (1.15.0)\n",
            "Installing collected packages: tensorflow-model-optimization\n",
            "Successfully installed tensorflow-model-optimization-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgPxeNJbxnuK"
      },
      "source": [
        "def data_loader(filepath):\r\n",
        "    data = h5py.File(filepath, 'r')\r\n",
        "    x_data = np.array(data['data'])\r\n",
        "    y_data = np.array(data['label'])\r\n",
        "    x_data = x_data.transpose((0,2,3,1))\r\n",
        "\r\n",
        "    return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyW5J3k1xq64"
      },
      "source": [
        "def data_preprocess(x_data):\r\n",
        "    return x_data/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wm11GSyxu75"
      },
      "source": [
        "def model_prune(x_train,y_train, bd_model):\r\n",
        "    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\r\n",
        "\r\n",
        "    # Compute end step to finish pruning after 2 epochs.\r\n",
        "    batch_size = 128\r\n",
        "    epochs = 3\r\n",
        "    validation_split = 0.1 # 10% of training set will be used for validation set.\r\n",
        "\r\n",
        "    num_images = x_train.shape[0] * (1 - validation_split)\r\n",
        "    end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\r\n",
        "\r\n",
        "    # Define model for pruning.\r\n",
        "    pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\r\n",
        "                                                                               final_sparsity=0.7,\r\n",
        "                                                                               begin_step=0,\r\n",
        "                                                                               end_step=end_step)}\r\n",
        "\r\n",
        "    model_for_pruning = prune_low_magnitude(bd_model, **pruning_params)\r\n",
        "\r\n",
        "    # `prune_low_magnitude` requires a recompile.\r\n",
        "    model_for_pruning.compile(optimizer='adam',\r\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\r\n",
        "                  metrics=['accuracy'])\r\n",
        "\r\n",
        "    model_for_pruning.summary()\r\n",
        "\r\n",
        "    #Train and evaluate the model against baseline\r\n",
        "    logdir = tempfile.mkdtemp()\r\n",
        "\r\n",
        "    callbacks = [\r\n",
        "      tfmot.sparsity.keras.UpdatePruningStep(),\r\n",
        "      tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\r\n",
        "    ]\r\n",
        "\r\n",
        "    model_for_pruning.fit(x_train, y_train,\r\n",
        "                      batch_size=batch_size, epochs=epochs, validation_split=validation_split,\r\n",
        "                      callbacks=callbacks)\r\n",
        "    return model_for_pruning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIytrIZlxyVW"
      },
      "source": [
        "def strip(x_test,y_test,N,model):\r\n",
        "    threshold = 0.1\r\n",
        "    random_pick = np.random.choice(len(x_test), N)\r\n",
        "    DT = x_test[random_pick]\r\n",
        "\r\n",
        "    boundary = []\r\n",
        "    judge1 = []\r\n",
        "    \r\n",
        "    for i in range(0, len(x_test), 50):\r\n",
        "        group = x_test[i:i + 50]\r\n",
        "        new_x = []\r\n",
        "        for xi in group:\r\n",
        "            for xt in DT:\r\n",
        "                xp = xi + xt\r\n",
        "                new_x.append(xp)\r\n",
        "        new_x = np.asarray(new_x)\r\n",
        "        res = model.predict(new_x)\r\n",
        "        for i in range(0, len(res), N):\r\n",
        "            Hsum = 0\r\n",
        "            r = res[i:i + N]\r\n",
        "            for y in r:\r\n",
        "                H = 0\r\n",
        "                for yi in y:\r\n",
        "                    if yi != 0:\r\n",
        "                        H -= yi * np.log2(yi)\r\n",
        "                Hsum += H\r\n",
        "            judge1.append(False) if Hsum / N < threshold else judge1.append(True)\r\n",
        "            boundary.append(Hsum / N)\r\n",
        "    print(sum(boundary)/len(boundary))\r\n",
        "    predict_label = np.argmax(model.predict(x_test), axis=1)\r\n",
        "    output_label1 = []\r\n",
        "    for i, v in enumerate(predict_label):\r\n",
        "        output_label1.append(v) if judge1[i] else output_label1.append(1283)\r\n",
        "    rate_p = output_label1.count(1283) / len(output_label1)\r\n",
        "    return(boundary,rate_p,output_label1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-_Wqyyrx28r"
      },
      "source": [
        "def strip_perIm(x_test,y_test,N,model,n_test):\r\n",
        "    threshold = 0.1\r\n",
        "    random_pick = np.random.choice(len(x_test), N)\r\n",
        "    DT = x_test[random_pick]\r\n",
        "    x_test_i = x_test[n_test]\r\n",
        "    new_x = []\r\n",
        "    for xt in DT:\r\n",
        "        xp = x_test_i + xt\r\n",
        "        new_x.append(xp)\r\n",
        "    new_x = np.asarray(new_x)\r\n",
        "    res = model.predict(new_x)\r\n",
        "    Hsum = 0\r\n",
        "    r = res\r\n",
        "    for y in r:\r\n",
        "        H = 0\r\n",
        "        for yi in y:\r\n",
        "            if yi != 0:\r\n",
        "                H -= yi * np.log2(yi)\r\n",
        "        Hsum += H\r\n",
        "    if Hsum / N < threshold:\r\n",
        "        output = 1283\r\n",
        "    else:\r\n",
        "        output = y_test[n_test]\r\n",
        "    return(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVkcFViXwaZv"
      },
      "source": [
        "#test_data_filename = 'lipstick_poisoned_data.h5'\r\n",
        "test_data_filename = 'clean_test_data.h5'\r\n",
        "train_data_filename = 'clean_validation_data.h5'\r\n",
        "model_filename = 'anonymous_2_bd_net.h5'\r\n",
        "n_test = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOuWcyheyEFe"
      },
      "source": [
        "x_test, y_test = data_loader(test_data_filename)\r\n",
        "x_test = data_preprocess(x_test)\r\n",
        "x_train, y_train = data_loader(train_data_filename)\r\n",
        "x_train = data_preprocess(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjj5CMPQyIeQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2175e5e-9ec7-4fa1-e0ae-b7d2515b0f08"
      },
      "source": [
        "bd_model = keras.models.load_model(model_filename)\r\n",
        "_, bd_model_accuracy_sun = bd_model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"baseline model accuracy is\",bd_model_accuracy_sun)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "baseline model accuracy is 0.0007794232224114239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6n9hEA4yKAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579a8524-645e-4e9b-b47d-6a259d45e28f"
      },
      "source": [
        "boundary_clean, rate_p_clean, predict_label_clean = strip(x_test, y_test, 10, bd_model)\r\n",
        "print(\"the fraction of poisoned data rate in this data is\",rate_p_clean)\r\n",
        "print(\"the predicted label for image %d to %d is\"%(n_test,n_test+10),predict_label_clean[n_test])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5294252988820123\n",
            "the fraction of poisoned data rate in this data is 0.00607950116913484\n",
            "the predicted label for image 5 to 15 is 788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56gVdIftyaAg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41432ff3-3f2a-4e19-ec76-98198f8fe2f7"
      },
      "source": [
        "# predict a single image in a dataset\r\n",
        "label = strip_perIm(x_test,y_test,10,bd_model,n_test)\r\n",
        "print(\"the class of the picture is %d\"%label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the class of the picture is 788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU-dw7FIycKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765cfed2-db1b-4258-9e6b-d271e115c6c5"
      },
      "source": [
        "# this is another method - pruning+finetuning, that we attempted to do.\r\n",
        "# this is parallel (unrelevant) to the previous strip method and might\r\n",
        "#   undermine the performance of strip. \r\n",
        "model_for_pruning = model_prune(x_train,y_train,bd_model)\r\n",
        "_, model_for_pruning_accuracy_sun = model_for_pruning.evaluate(x_test, y_test, verbose=0)\r\n",
        "print(\"pruned model accuracy is\",model_for_pruning_accuracy_sun)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              [(None, 55, 47, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_1 (Pru (None, 52, 44, 20)   1942        input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_pool_1 (Pru (None, 26, 22, 20)   1           prune_low_magnitude_conv_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_2 (Pru (None, 24, 20, 40)   14442       prune_low_magnitude_pool_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_pool_2 (Pru (None, 12, 10, 40)   1           prune_low_magnitude_conv_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_3 (Pru (None, 10, 8, 60)    43262       prune_low_magnitude_pool_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_pool_3 (Pru (None, 5, 4, 60)     1           prune_low_magnitude_conv_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_conv_4 (Pru (None, 4, 3, 80)     38482       prune_low_magnitude_pool_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_flatten_1 ( (None, 1200)         1           prune_low_magnitude_pool_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_flatten_2 ( (None, 960)          1           prune_low_magnitude_conv_4[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_fc_1 (Prune (None, 160)          384162      prune_low_magnitude_flatten_1[0][\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_fc_2 (Prune (None, 160)          307362      prune_low_magnitude_flatten_2[0][\n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_add_1 (Prun (None, 160)          1           prune_low_magnitude_fc_1[0][0]   \n",
            "                                                                 prune_low_magnitude_fc_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_activation_ (None, 160)          1           prune_low_magnitude_add_1[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "prune_low_magnitude_output (Pru (None, 1283)         411845      prune_low_magnitude_activation_1[\n",
            "==================================================================================================\n",
            "Total params: 1,201,504\n",
            "Trainable params: 601,643\n",
            "Non-trainable params: 599,861\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/3\n",
            " 6/82 [=>............................] - ETA: 4s - loss: 0.5315 - accuracy: 0.8835WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0139s vs `on_train_batch_end` time: 0.0291s). Check your callbacks.\n",
            "82/82 [==============================] - 6s 30ms/step - loss: 0.3585 - accuracy: 0.9198 - val_loss: 0.2303 - val_accuracy: 0.9411\n",
            "Epoch 2/3\n",
            "82/82 [==============================] - 2s 21ms/step - loss: 0.2934 - accuracy: 0.9257 - val_loss: 0.3331 - val_accuracy: 0.9160\n",
            "Epoch 3/3\n",
            "82/82 [==============================] - 2s 21ms/step - loss: 0.1325 - accuracy: 0.9681 - val_loss: 0.4101 - val_accuracy: 0.8952\n",
            "pruned model accuracy is 0.9107560515403748\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
